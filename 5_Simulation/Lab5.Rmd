---
title: 'Lab 5: power analysis with occupancy models'
author: "Ben Goldstein and Krishna Pacifici"
date: "Fall 2024"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

# Install/load the necessary packages and 
tryCatch(library(devtools), 
         error = function(e) {
           install.packages("devtools")
           library(devtools)
         })
tryCatch(library(lubridate), 
         error = function(e) {
           install.packages("lubridate")
           library(lubridate)
         })
tryCatch(library(EcoData), 
         error = function(e) {
           devtools::install_github(repo = "TheoreticalEcology/EcoData", 
                                    dependencies = T, build_vignettes = T)
           library(EcoData)
         })
tryCatch(library(tidyverse), 
         error = function(e) {
           install.packages("tidyverse")
           library(tidyverse)
         })
tryCatch(library(nimble), 
         error = function(e) {
           install.packages("nimble")
           library(nimble)
         })
tryCatch(library(ubms), 
         error = function(e) {
           install.packages("ubms")
           library(ubms)
         })
```


In this lab, we'll explore one case where you might want to use data simulation
in the real world---a power analysis.

Imagine that you're planning to conduct a field study to investigate the 
occupancy rates of a hard-to-detect woodpecker, and how distance to forest edge 
influences the occupancy of the woodpecker. For your field study, you identify 
5 forests where you can sample sites along a gradient from edge to forest 
interior. You want to decide how many sites to place along that gradient, and
how many samples you want to collect per site, so that you have adequate **power**.

> What do we mean by "power" in this context? 
> How is it related to type 1 and/or type 2 error?

We will keep things relatively simple for this example, but if you were doing 
this for a real field study, it'd be a good idea to introduce any realistic 
details you can anticipate.


### The general approach

The idea behind power analysis is to simulate data under a given model with different conditions (i.e., different true effect sizes) and track the rate at which we're able to 

Here, I illustrate this with a brief power analysis using a linear model defined by the equation

$$y_i \sim \mathcal{N} (\beta_0 + \beta_1 x_i, \sigma^2)$$

I'll simulate datasets of different sizes, fit the linear model, and track the rate at which we detect the true relationship that $\beta_1 \neq 0$.

```{r}
set.seed(1001)

### Run where the true effect size is 0.5
true_beta <- 0.5
true_mean <- 1
true_error_sd <- 1

ndat <- c(5, 10, 15, 20, 25, 30, 500)
nrep <- 1000

power1 <- numeric(length(ndat))

for (i in 1:length(ndat)) {
  significant <- logical(nrep)
  for (j in 1:nrep) {
    x <- runif(ndat[i], -1, 1)
    y <- rnorm(ndat[i], true_mean + true_beta * x, true_error_sd)
    
    this_fit <- lm(y ~ x)
    s <- summary(this_fit)
    
    # is the p-value for the slope significant?
    significant[j] <- s$coefficients[2, 4] < 0.05
  }
  power1[i] <- mean(significant) # Power is the rate at which we find an effect
}

### Repeat with true_beta = 0.2
true_beta <- 5
power2 <- numeric(length(ndat))

for (i in 1:length(ndat)) {
  significant <- logical(nrep)
  for (j in 1:nrep) {
    x <- runif(ndat[i], -1, 1)
    y <- rnorm(ndat[i], true_mean + true_beta * x, true_error_sd)
    
    this_fit <- lm(y ~ x)
    s <- summary(this_fit)
    
    # is the p-value for the slope significant?
    significant[j] <- s$coefficients[2, 4] < 0.05
  }
  power2[i] <- mean(significant) # Power is the rate at which we find an effect
}

bind_rows(
  data.frame(
      power = power1,
      ndat = ndat,
      beta = 0.5
  ),
  data.frame(
      power = power2,
      ndat = ndat,
      beta = 5
  )) %>% 
  ggplot() +
  geom_line(aes(ndat, power, group = beta, color = as.factor(beta))) +
  theme_minimal()

```





### The model equations

The first thing we want to do is specify a set of model equations that we'll 
simulate from. We'll assume that these equations will represent the real data
generating process during our simulation.

We'll start by defining a basic occupancy model with site-specific occupancy
probability and visit-specific detection probability:

$$y_{ij} \sim \text{Bernoulli}(z_i p_{ij})$$
$$z_i \sim \text{Bernoulli}(\psi_i)$$

Now we need to define relationships for detection and occupancy probabilities.
We should include all the variables we want to measure either as effects of 
interest or potential confounders. 

For now, we'll keep it simple and say that we're only looking at distance to
forest edge as an effect on occupancy, but we also want to account for a 
potential random effect of forest because there may be other unobserved features
that mean occupancy is different between our different transects. We'll define occupancy probability as

$$\text{logit}(\psi_i) = \beta_0 + \beta_1 x_{i} + \epsilon_{f(i)}$$

> What other potential features (covariates, groupings, etc.) might you want
> to include in an analysis like this one?


We'll start with a simple detection model, where the detection probability is 
the same for all sites and visits, so:

$$p_{ij} = p_0$$

### Write an R function to simulate data

We'll write a function to simulate occupancy data of the type we expect to 
collect, i.e., following the equations we just wrote out above.

Since we're going to be doing this a bunch of times, it's a good idea to wrap 
this inside a function that we can call. Since we want to be able to flexibly 
change the parameters of the simulation, we'll want to make sure the function 
has arguments that allow us to specify useful values.

In our case, we will adjust the total number of sites, the number of replicates 
per site, and the true effect of distance to forest on occupancy, but we might 
want to expand this list later on.

```{r}
simulate_data <- function(nsites_per_forest, 
                          nreps,
                          true_forest_effect, 
                          true_forest_sd,
                          logit_occupancy_intercept,
                          logit_detection_intercept) {
  
  dist_edge <- seq(from = 0, to = 2000, length.out = nsites_per_forest)
  
  siteCovs <- as.data.frame(expand.grid(
    dist_edge = dist_edge, 
    forest_ID = 1:5
  )) %>% 
    mutate(site_ID = row_number())
  
  # Scale distance-to-edge
  siteCovs$dist_edge = as.numeric(scale(siteCovs$dist_edge))
  
  # Simulate the forest random effect
  forest_ranef <- rnorm(n = 5, mean = 0, sd = true_forest_sd)
  
  # Calculate the occupancy prob for each site
  psi <- expit(
    logit_occupancy_intercept + true_forest_effect * siteCovs$dist_edge +
      forest_ranef[siteCovs$forest_ID]
  )
  
  p <- expit(logit_detection_intercept)
  
  z <- numeric(nrow(siteCovs))
  y <- matrix(NA, nrow = nrow(siteCovs), ncol = nreps)
  
  for (i in 1:nrow(y)) {
    # Simulate the latent state
    z[i] <- rbinom(n = 1, size = 1, prob = psi[i])
    
    y[i,] <- rbinom(n = nreps, size = 1, prob = z[i] * p)
  }
  
  return(list(
    # Return the observed data
    y = y,
    siteCovs = siteCovs,
    umf = unmarkedFrameOccu(y = y, siteCovs = siteCovs),

    # Also return the inputs so we can easily track them later
    sim_specification = data.frame(
      nsites_per_forest = nsites_per_forest,
      nreps = nreps,
      true_forest_effect = true_forest_effect,
      true_forest_sd = true_forest_sd,
      logit_occupancy_intercept,
      logit_detection_intercept
    )
  ))
}
```


### Simulate and estimate one model

Let's test our whole workflow with a single realized dataset. For the test,
we'll imagine surveying 5 sites in each of 5 forests and conducting 6 replicate
visits to each site, for a total of 150 surveys. 

```{r}
set.seed(6644)

datlist <- simulate_data(nsites_per_forest = 5, 
                         nreps = 6, 
                         true_forest_effect = 1, 
                         true_forest_sd = 0.2, 
                         logit_occupancy_intercept = 0, 
                         logit_detection_intercept = 0)

# View the data matrix (red = detected)
image(datlist$y)
```

We'll use the package `ubms` to estimate the occupancy model. `ubms` uses one-line
calls like unmarked, but estimates models via MCMC and allows random effects.

```{r}
# ubms

# Fit an occupancy model with no det covariates, with dist. to forest edge
# and forest ID random effect on occu
silence <- capture.output(
  test_fit <- stan_occu(~ 1 ~ dist_edge + (1|forest_ID), data = datlist$umf,
                      iter = 5000, warmup = 2500, verbose = FALSE, refresh=-1)
)
summary(test_fit, submodel = "state")
summary(test_fit, submodel = "det")

```

To test power, we'll monitor the rate at which we detect the true nonzero effect
of distance to forest edge. We don't have p-values to extract, but we can check
whether the 

Obviously there's a lot more going on that we could be monitoring between 
simulations, such as the precision of the estimate, bias in the estimate, 
estimates of detection, etc., and in a real power analysis that'd be a good idea.

### Function to evaluate one model's performance

Let's wrap this whole process in a function so our code looks cleaner.

```{r}
fit_and_evaluate_one <- function(this_datlist) {
  silence <- suppressWarnings(capture.output(
    this_fit <- stan_occu(~ 1 ~ dist_edge + (1|forest_ID), data = this_datlist$umf,
                        iter = 4000, warmup = 2000, verbose = FALSE, refresh=-1)
  ))
  
  s <- summary(this_fit, "state")
  
  # Is the credible interval different from 0
  return(sign(s$`2.5%`)[2] == sign(s$`97.5%`)[2])
}

```


### Run the power analysis: number of sites vs. number of reps

Now we can use the same approach as the LM example to run a power analysis 
across different conditions. I'll set up a data frame of potential conditions


```{r}
set.seed(229983)

conditions <- data.frame(
  nsites_per_forest = c(5, 6, 10, 15),
  nreps = c(6, 5, 3, 2)
) %>% 
  mutate(nsurvey_tot = nsites_per_forest * nreps * 5,
         power = NA)

conditions

# Keep this low. In reality, we need way more, but it's slow to estimate these
nsim <- 25

for (i in 1:nrow(conditions)) {
  significant <- logical(nsim)
  for (j in 1:nsim) {
    datlist <- simulate_data(nsites_per_forest = conditions$nsites_per_forest[i],
                             nreps = conditions$nreps[i], 
                             true_forest_effect = 1, 
                             true_forest_sd = 0.25, 
                             logit_occupancy_intercept = 0, 
                             logit_detection_intercept = 0)
    
    significant[j] <- fit_and_evaluate_one(datlist)
  }
  conditions$power[i] <- mean(significant)
}

# We need more simulations for this to be stable
conditions 
```


### Exercise: How does power change with diff. detection proabilities?

Using the preceding code as a starting point, evaluate how power to detect
an effect of distance to forest edge on woodpecker occupancy changes for different potential mean detection probabilities.

HINT: you can use the same functions for data simulation and model fitting.

```{r}

```


### Exercise: what if distance to forest edge affects detection, too?

Using the preceding code as a starting point, evaluate how power to detect
an effect of distance to forest edge on woodpecker occupancy changes for a
case where detection probability also depends on distance to forest edge.

Specifically, simulate such that 
$$\text{logit}(p_{ij}) = \alpha_0 + \alpha_1 x_i$$
where $\alpha_1 = 0.5$.

HINT: you will need to modify both the data simulation function and the model-fitting function!

```{r}

```

