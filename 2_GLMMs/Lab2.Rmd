---
title: 'Lab 2: Generalized linear mixed models'
author: "Ben Goldstein and Krishna Pacifici"
date: "Fall 2024"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

source("../helper/Prepare_EcoData.R")

library(tidyverse)
```


## Simulating data from a GLMM

One way to learn about a model's behavior is to simulate data from the 
model. Then, you can look at the simulated data to start to build up
an intuition about the role of the parameters and where the information
in the observed data lives.

Let's conduct such an exercise with a GLMM, using a classic example where
excluding random effects can dramatically change the inference.

In this example, we want to know how soil moisture effects the growth
rate of a plant. Specifically, we'll grow 100 plants in varying moisture 
conditions, then record the number of fruit produced at the end of the
season. Due to practical constraints, we'll grow 20 plants in each of 
5 fields---a source of nonindependence in the data that we'll want to 
model with random effects. We'll model these fruit counts as a Poisson
random variable with a mean value log-linked to average soil moisture
over the growing season.

We want to simulate data from the GLMM correctly. To do that, we need to
know what equations we want to use. Our data will be generated from the
following GLMM:
$$n_i \sim \text{Poisson}(\lambda_i)$$
$$\log(\lambda_i) = \beta_0 + \beta_1 \times \text{moisture}_i + \alpha_{f(i)}$$
$$\alpha_f \sim \mathcal{N}(0, \sigma)$$
where $n_i$ is the count of fruit associated with plant $i$; $\lambda_i$ is the expected number of fruit associated with plant $i$; $\beta_0$ is the intercept term, interpretable as the log of the expected number of fruit when the moisture level is 0 (or at mean conditions if moisture is scaled); $\beta_1$ is the effect of increased moisture on the log-scale expected number of fruit; $\alpha_{f(i)}$ is a normally distributed random effect of whichever field $f$ contains plant $i$; and $\sigma$ is the standard deviation of the normal random effect.

It sounds like there's a lot going on, but when we go to simulate the data, it'll end up being relatively simple. So, let's do that!

```{r}
# Set a seed for replicability
set.seed(59670)

# Number of plants, number of fields
n_plants <- 100
n_fields <- 5

# create a vector of IDs indicating which field contained each plant.
field <- rep(1:n_fields, each = n_plants / n_fields)

# Simulate a moisture level for each field. The range of moisture is not even across each field. 
# Keep in mind that a linear model does not make any assumptions about the distribution of covariate data!
moisture <- c(runif(n_plants / n_fields, 2, 12),
              runif(n_plants / n_fields, 5, 15),
              runif(n_plants / n_fields, 6, 18),
              runif(n_plants / n_fields, 8, 18),
              runif(n_plants / n_fields, 9, 18))

# Center and scale these for convenience
moisture <- as.numeric(scale(moisture))

# So far we've just simulated moisture_i arbitrarily. Now let's get to the model. 
# We need to pick values for true parameters. (Remember these are log-scale)
beta0 <- 1.5
beta1 <- 0.6
sigma <- 1.1

# We need to simulate a random effect level for each field
alpha <- rnorm(n = n_fields, mean = 0, sd = sigma)
alpha

# Now we can calculate lambda deterministically, following the model eqn
log_lambda <- beta0 + beta1 * moisture + alpha[field] # <- nested indexing
lambda <- exp(log_lambda)

# Finally, draw random fruit counts from the Poisson
n_fruit <- rpois(n = n_plants, lambda)

# Put these in a data.frame
dat <- data.frame(
  plant_ID = 1:n_plants,
  field = field,
  n_fruit = n_fruit,
  moisture_scaled = moisture,
  true_log_lambda = log_lambda
)

head(dat)


# Let's plot the data!
ggplot(dat, aes(moisture_scaled, n_fruit)) +
  geom_point() +
  xlab("Moisture") + ylab("Number of fruit") +
  theme_minimal() +
  geom_smooth(method = "lm")

```

Doesn't look like there's much of a relationship between these variables in the simulated data. But if we explore how these results vary by field, the relationship becomes clearer.

```{r}
# Let's plot the data!
ggplot(dat,
       aes(moisture_scaled, n_fruit, 
                 col = as.factor(field)), group = field) +
  geom_point() +
  xlab("Moisture") + ylab("Number of fruit") +
  theme_minimal() +
  geom_smooth(method = "lm")

```

Within each field, there's a positive relationship between moisture and number of fruit, but this is obscured due to the variation betwen fields. There are some field-specific factors that apparently make growing much worse in certain fields.

Notice how the log link function used to generate the data distorts the seemingly linear relationship across groups.

Let's compare estimates from a GLM (which ignores the random effect) vs. a GLMM (which includes the random effect):

```{r}
glm_fit <- glm(n_fruit ~ moisture_scaled, family = "poisson", data = dat)
summary(glm_fit)

library(lme4)
glmm_fit <- glmer(n_fruit ~ moisture_scaled + (1 | field), 
                  family = "poisson", data = dat)
summary(glmm_fit)



dat$predicted <- predict(glmm_fit, type = "link")

ggplot(dat,
       aes(moisture_scaled, n_fruit, 
                 col = as.factor(field)), group = field) +
  # geom_point() +
  geom_line(mapping = aes(moisture_scaled, predicted, col = as.factor(field))) +
  xlab("Moisture") + ylab("Number of fruit") +
  theme_minimal()


```

In this case, the GLM correctly identified a positive relationship, but it underestimated the effect size, while the GLMM did a better job but still ultimately produced an underestimate.

## Bayesian estimation of a GLMM

### Back to beetles

Let's return to the beetles survey data from Lab 1. Recall that beetles were surveyed at a number of locations

```{r}
beetle_data <- EcoData::volcanoisland
beetle_data$altitude_scaled <- as.numeric(scale(beetle_data$altitude))

head(beetle_data)

```

**What sources of potential nonindependence did we ignore in the previous analysis?**



### A GLMM is a GLM plus random effects

Let's build a GLMM by modifying the Lab 1 model code. The old model code
is here:

```{r}
library(nimble)

GLM_code <- nimbleCode({
  
  # Data likelihood
  for (i in 1:nobs) {
    log(mu[i]) <- beta0 + beta1 * altitude[i] + beta2 * altitude[i]^2
    y[i] ~ dpois(mu[i])
  }
  
  # Priors for our parameters
  beta0 ~ dnorm(0, sd = 10)
  beta1 ~ dnorm(0, sd = 10)
  beta2 ~ dnorm(0, sd = 10)
})

```

Now, we want to add in a random intercept effect of each plot.

```{r}
GLMM_code <- nimbleCode({
  # Data likelihood
  for (i in 1:nobs) {
    log(mu[i]) <- beta0 + beta1 * altitude[i] + #beta2 * altitude[i]^2 +
                    ranef_plot[plot[i]]
    y[i] ~ dpois(mu[i])
  }
  
  # Priors for our parameters
  beta0 ~ dnorm(0, sd = 10)
  beta1 ~ dnorm(0, sd = 10)
  beta2 ~ dnorm(0, sd = 10)
  sigma ~ dunif(0, 10)
  
  # Random effect distribution
  for (i in 1:n_plot) {
    ranef_plot[i] ~ dnorm(0, sd = sigma)
  }
})

```

Notice how the modification we made to the `log(mu[i])` calculation looks 
a lot like what we did when we simulated data.

Let's go ahead and estimate the model with MCMC as we did in the last lab.
I'll indicate with comments where I've added new lines.

```{r}
my_GLMM <- nimbleModel(
  code = GLMM_code,
  data = list(
    altitude = beetle_data$altitude_scaled,
    y = beetle_data$beetles
  ),
  constants = list(
    nobs = nrow(beetle_data),
    n_plot = length(unique(beetle_data$plot)),
    plot = as.numeric(beetle_data$plot) # plot
  ),
  inits = list(
    beta0 = 0,
    beta1 = 1,
    beta2 = 1,
    sigma = 1, # new
    ranef_plot = rnorm(length(unique(beetle_data$plot)), 0, 1) # new
  )
)
```

I'll briefly note that, since plot is used as an index, it's very important
that it's provided as an integer starting at 1. Conveniently, it's already
formatted that way, but if it weren't; for example, if it were provided
some string (e.g. A1, A2...), we'd need to convert it ourselves.

```{r}
mcmcConf <- configureMCMC(my_GLMM)
mcmc <- buildMCMC(mcmcConf)

compiled_list <- compileNimble(my_GLMM, mcmc)

samples <- runMCMC(compiled_list$mcmc, niter = 10000, nburnin = 2000,
                   thin = 1, nchains = 3, samplesAsCodaMCMC = TRUE)

summary <- MCMCvis::MCMCsummary(samples)

summary

plot(samples[, 4])

```




## Exercise: Estimating a GLMM to explain iNaturalist reporting rates

Let's apply what we've learned about GLMMs to a new dataset. 
In this example, we'll use a slightly nontradition version of a GLMM---
one that we'll need NIMBLE to estimate.

[iNaturalist](inaturalist.org) is a participatory science repository that
collects amateur observations from around the world.

I've processed some iNaturalist data and obtained reported counts of 
eastern gray squirrels in each of many grid cells. 


```{r}
# Download the file from github
download.file("https://raw.githubusercontent.com/dochvam/AHM_Labs_2024/main/data/inat_EGSq_dat.csv", 
              "inat_EGSq_dat.csv")

inat_data <- read_csv("inat_EGSq_dat.csv")

head(inat_data)
```
Each row corresponds to a cell/year of data. The following columns are provided:

- `grid_cell` is a unique ID for each 50 km by 50 km spatial cell.
- `year` is the year across which observations were aggregated.
- `total` is the number of iNaturalist mammal observations in each cell/year, a measure of effort.
- `squirrel` is the number of observations of eastern gray squirrels in each cell/year
- `EVI_mean` is the mean EVI in the cell. *NOT* time-varying; all covariates are averages
- `Human_popden_sqrt` is the square root of average human population density in the cell
- `Precipitation` is the average precipitation level in each cell
- `Temp_max` is the average annual max daily temperature in the cell

We want to understand how squirrel counts, per unit effort, vary according to four covariates: EVI, precipitation, and maximum daily temperature. We also want to account for two sources of nonindependence: space and time. We'll include crossed grouping random effects of grid cell and year.

The model we want to estimate is going to be defined by the following equations. First, the observed count of squirrels in cell/year $i$, $y_i$, is distributed as
$$y_i \sim \text{Poisson}(\text{N}_i \mu_i)$$
where $N_i$ is the **observed** total number of iNaturalist observations of mammals in the cell year (effort) and $mu_i$ is the expected number of observations of eastern gray squirrels in cell $i$. Then,
$$\log(\mu_i) = \beta_0 + \beta_1 \times \text{EVI}_i + \beta_2 \times \text{Temp}_i +
\beta_3 \times \text{Precip}_i + \alpha_{\text{year}(i)} + \gamma_{\text{cell}(i)}$$
and 
$$\alpha_{\text{year}} \sim \mathcal{N}(0, \sigma_{\text{year}})$$
$$\gamma_{\text{cell}} \sim \mathcal{N}(0, \sigma_{\text{cell}})$$
So what we have here is a GLMM with two levels of random effects that also includes an effort offset.

Let's attempt to implement this model in NIMBLE.

```{r}

iNat_GLMM_code <- nimbleCode({

  # Code goes here
  for (i in 1:nobs) {
    # Deterministic calculation of the expected per-obs. reporting rate
    log(mu[i]) <- beta0 + beta1 * evi[i] +
      beta2 * temp[i] + beta3 * precip[i] + alpha[year[i]] + gamma[cell[i]]
    
    # probability distribution for y; observed number of obs. (N) times per-obs
    # reporting rate (mu) is the expected value
    y[i] ~ dpois(mu[i] * N[i])
  }
  
  # Random effects distributions
  for (i in 1:nyear) {
    alpha[i] ~ dnorm(0, sd = sigma_year)
  }
  for (i in 1:ncell) {
    gamma[i] ~ dnorm(0, sd = sigma_cell)
  }
  
  
  # Priors for the parameters we want to estimate
  beta0 ~ dnorm(0, sd = 5)
  beta1 ~ dnorm(0, sd = 5)
  beta2 ~ dnorm(0, sd = 5)
  beta3 ~ dnorm(0, sd = 5)
  sigma_year ~ dgamma(1, 1)
  sigma_cell ~ dgamma(1, 1)
})

```


Now, build the model.

```{r}
iNat_GLMM_model <- nimbleModel(
  iNat_GLMM_code,
  constants = list(
    # Data dimensions
    nobs = nrow(inat_data),
    nyear = length(unique(inat_data$year)),
    ncell = length(unique(inat_data$grid_cell)),
    # Nested indexes for random effects. We need to manipulate these to make sure
    # they go from 1:N.
    year = inat_data$year - 1999,
    cell = as.numeric(as.factor(inat_data$grid_cell)) # <- trick for getting these on 1:N
  ),
  data = list(
    y = inat_data$squirrel,
    N = inat_data$total,
    evi = as.numeric(scale(inat_data$EVI_mean)),
    temp = as.numeric(scale(inat_data$Temp_max)),
    precip = as.numeric(scale(inat_data$Precipitation))
  ), 
  inits = list(
    beta0 = 1, beta1 = 1, beta2 = 1, beta3 = 1,
    sigma_year = 1, sigma_cell = 1,
    gamma = rnorm(length(unique(inat_data$grid_cell)), 0, 1),
    alpha = rnorm(length(unique(inat_data$year)), 0, 1)
  )
)
```

Finally, add the rest of the model estimation workflow here. I'm copy/pasting
what we've now seen a bunch of times and making sure that I'm passing in the
correct model.

Expect this to run more slowly than the examples up til now, for two reasons.
One is that the dataset is bigger than anything we've worked with. The other
reason is that the MCMC algorithm has to draw samples for every individual random 
effect, so by including a random effect in this model, we've gone from having
5 samplers (betas) to 100s (betas + sigmas + individual levels for each year/cell).

For that reason, I'll lower the number of iterations, which could mean that the
model doesn't converge in the time allowed. I'll have to check carefully
that the model has converged before I interpret the parameter estimates! If it
hasn't, I'll just increase the number of iterations and re-run the MCMC algorithm
until it has.

```{r}
mcmcConf <- configureMCMC(iNat_GLMM_model)
mcmc <- buildMCMC(mcmcConf)

compiled_list <- compileNimble(iNat_GLMM_model, mcmc)

samples <- runMCMC(compiled_list$mcmc, niter = 8000, nburnin = 1000,
                   thin = 1, nchains = 3, samplesAsCodaMCMC = TRUE)

summary <- MCMCvis::MCMCsummary(samples)

summary

```



